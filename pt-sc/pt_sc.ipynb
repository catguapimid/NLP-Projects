{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import jieba\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "#define the vocab class for each language\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in jieba.cut(sentence, cut_all=False, HMM=True):  #jieba分词\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s) #NFD表示字符应该分解为多个组合字符表示\n",
    "        if unicodedata.category(c) != 'Mn'         #去掉音调符号\n",
    "    )\n",
    "\n",
    "#trim, and remove non-Chinese characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    import re\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？?、~@#￥%……&*（）]+\", \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    print(len(lines))\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    print('pairs: %d'% len(pairs))\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "11265\n",
      "pairs: 11265\n",
      "Read 11265 sentence pairs\n",
      "Trimmed to 11265 sentence pairs\n",
      "Counting words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.437 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "sc 6696\n",
      "pt 7079\n",
      "['在下一个转角右转', '在下一个转角右转']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])  #construct the input_lang vocab\n",
    "        output_lang.addSentence(pair[1])  #construct the output_lang vocab\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('pt', 'sc', True)  #普通话-四川话\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the encoder-decoder translation architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, in_embed_dim): #input_size: input_lang vocab size\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, in_embed_dim) \n",
    "        self.gru = nn.GRU(in_embed_dim, hidden_size) \n",
    "\n",
    "    def forward(self, input, hidden):                    #input and h_0\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, out_embed_dim, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.out_embed_dim = out_embed_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.out_embed_dim) \n",
    "        self.attn = nn.Linear(self.hidden_size + self.out_embed_dim, self.max_length) \n",
    "        self.attn_combine = nn.Linear(self.hidden_size + self.out_embed_dim, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size) #output_size: output_lang vocab size\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #why use 0 ? embedded[0], hidden[0]..., because it's the init state\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0)) #weighted_sum\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train, evaluate and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    #lang: language vocab object\n",
    "    return [lang.word2index[word] for word in jieba.cut(sentence)]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)  #append the EOS_token, it's crucial\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) #初始化为全0\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0] # encoder_output[0, 0] means  encoder_output[0][0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device) #1*1\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = s // 60\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)] #迭代n_iters次，每次随机从pairs中选一对\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion) #training\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) #初始化为全0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0] #注意是累加，而train中是赋值\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)    #初始化attention为全0\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')  #解码时遇到EOS_token，要将其加入到decoded_words,而在train中编码时，直接break\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()]) #加入解码的词\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  #next decoder input\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1] #返回attentions可用来做可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1]) \n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ''.join(output_words) #机器翻译,中文以空字符连接，英文以空格连接\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 17s (- 28m 4s) (1000 1%) 5.0954\n",
      "0m 24s (- 19m 48s) (2000 2%) 4.9560\n",
      "0m 31s (- 17m 4s) (3000 3%) 4.7455\n",
      "0m 39s (- 15m 44s) (4000 4%) 4.4024\n",
      "0m 46s (- 14m 50s) (5000 5%) 4.2240\n",
      "0m 54s (- 14m 12s) (6000 6%) 4.0779\n",
      "1m 1s (- 13m 43s) (7000 7%) 3.9015\n",
      "1m 9s (- 13m 21s) (8000 8%) 3.8624\n",
      "1m 17s (- 13m 1s) (9000 9%) 3.6885\n",
      "1m 25s (- 12m 45s) (10000 10%) 3.6106\n",
      "1m 32s (- 12m 31s) (11000 11%) 3.5253\n",
      "1m 40s (- 12m 16s) (12000 12%) 3.3539\n",
      "1m 48s (- 12m 4s) (13000 13%) 3.1009\n",
      "1m 56s (- 11m 52s) (14000 14%) 3.1515\n",
      "2m 3s (- 11m 42s) (15000 15%) 3.0632\n",
      "2m 11s (- 11m 32s) (16000 16%) 2.9201\n",
      "2m 19s (- 11m 21s) (17000 17%) 2.8737\n",
      "2m 27s (- 11m 11s) (18000 18%) 2.7938\n",
      "2m 35s (- 11m 2s) (19000 19%) 2.6918\n",
      "2m 43s (- 10m 52s) (20000 20%) 2.5874\n",
      "2m 50s (- 10m 42s) (21000 21%) 2.5747\n",
      "2m 58s (- 10m 33s) (22000 22%) 2.4601\n",
      "3m 6s (- 10m 24s) (23000 23%) 2.5331\n",
      "3m 14s (- 10m 16s) (24000 24%) 2.3646\n",
      "3m 22s (- 10m 7s) (25000 25%) 2.1649\n",
      "3m 30s (- 9m 58s) (26000 26%) 2.2796\n",
      "3m 38s (- 9m 49s) (27000 27%) 2.2615\n",
      "3m 45s (- 9m 40s) (28000 28%) 2.1050\n",
      "3m 53s (- 9m 32s) (29000 28%) 2.0555\n",
      "4m 1s (- 9m 24s) (30000 30%) 2.0831\n",
      "4m 9s (- 9m 15s) (31000 31%) 2.0780\n",
      "4m 17s (- 9m 7s) (32000 32%) 1.9601\n",
      "4m 25s (- 8m 59s) (33000 33%) 1.9984\n",
      "4m 33s (- 8m 50s) (34000 34%) 1.9058\n",
      "4m 41s (- 8m 42s) (35000 35%) 1.8151\n",
      "4m 49s (- 8m 34s) (36000 36%) 1.8002\n",
      "4m 57s (- 8m 25s) (37000 37%) 1.8924\n",
      "5m 5s (- 8m 17s) (38000 38%) 1.7577\n",
      "5m 12s (- 8m 9s) (39000 39%) 1.7417\n",
      "5m 20s (- 8m 1s) (40000 40%) 1.6924\n",
      "5m 29s (- 7m 53s) (41000 41%) 1.6519\n",
      "5m 37s (- 7m 45s) (42000 42%) 1.5994\n",
      "5m 45s (- 7m 37s) (43000 43%) 1.5059\n",
      "5m 53s (- 7m 29s) (44000 44%) 1.4982\n",
      "6m 1s (- 7m 21s) (45000 45%) 1.5521\n",
      "6m 9s (- 7m 14s) (46000 46%) 1.4692\n",
      "6m 18s (- 7m 6s) (47000 47%) 1.3527\n",
      "6m 26s (- 6m 58s) (48000 48%) 1.4089\n",
      "6m 34s (- 6m 51s) (49000 49%) 1.2918\n",
      "6m 43s (- 6m 43s) (50000 50%) 1.3284\n",
      "6m 51s (- 6m 35s) (51000 51%) 1.2932\n",
      "6m 59s (- 6m 27s) (52000 52%) 1.2824\n",
      "7m 8s (- 6m 19s) (53000 53%) 1.1876\n",
      "7m 16s (- 6m 11s) (54000 54%) 1.1187\n",
      "7m 24s (- 6m 3s) (55000 55%) 1.1873\n",
      "7m 33s (- 5m 55s) (56000 56%) 1.0984\n",
      "7m 41s (- 5m 48s) (57000 56%) 1.0863\n",
      "7m 49s (- 5m 40s) (58000 57%) 1.1219\n",
      "7m 58s (- 5m 32s) (59000 59%) 1.0345\n",
      "8m 6s (- 5m 24s) (60000 60%) 0.9732\n",
      "8m 14s (- 5m 16s) (61000 61%) 0.9919\n",
      "8m 23s (- 5m 8s) (62000 62%) 0.9491\n",
      "8m 31s (- 5m 0s) (63000 63%) 0.9687\n",
      "8m 39s (- 4m 52s) (64000 64%) 0.9293\n",
      "8m 48s (- 4m 44s) (65000 65%) 0.8616\n",
      "8m 56s (- 4m 36s) (66000 66%) 0.8688\n",
      "9m 4s (- 4m 28s) (67000 67%) 0.8646\n",
      "9m 12s (- 4m 19s) (68000 68%) 0.8120\n",
      "9m 20s (- 4m 11s) (69000 69%) 0.8164\n",
      "9m 28s (- 4m 3s) (70000 70%) 0.7954\n",
      "9m 36s (- 3m 55s) (71000 71%) 0.7632\n",
      "9m 44s (- 3m 47s) (72000 72%) 0.7784\n",
      "9m 52s (- 3m 39s) (73000 73%) 0.7379\n",
      "10m 0s (- 3m 31s) (74000 74%) 0.7210\n",
      "10m 8s (- 3m 22s) (75000 75%) 0.7224\n",
      "10m 16s (- 3m 14s) (76000 76%) 0.7193\n",
      "10m 25s (- 3m 6s) (77000 77%) 0.6460\n",
      "10m 33s (- 2m 58s) (78000 78%) 0.6298\n",
      "10m 41s (- 2m 50s) (79000 79%) 0.6387\n",
      "10m 49s (- 2m 42s) (80000 80%) 0.6692\n",
      "10m 58s (- 2m 34s) (81000 81%) 0.6417\n",
      "11m 6s (- 2m 26s) (82000 82%) 0.6046\n",
      "11m 14s (- 2m 18s) (83000 83%) 0.5609\n",
      "11m 22s (- 2m 10s) (84000 84%) 0.5191\n",
      "11m 31s (- 2m 1s) (85000 85%) 0.5391\n",
      "11m 39s (- 1m 53s) (86000 86%) 0.5222\n",
      "11m 47s (- 1m 45s) (87000 87%) 0.5032\n",
      "11m 55s (- 1m 37s) (88000 88%) 0.4716\n",
      "12m 4s (- 1m 29s) (89000 89%) 0.4344\n",
      "12m 12s (- 1m 21s) (90000 90%) 0.4835\n",
      "12m 20s (- 1m 13s) (91000 91%) 0.3963\n",
      "12m 29s (- 1m 5s) (92000 92%) 0.4535\n",
      "12m 37s (- 0m 57s) (93000 93%) 0.4240\n",
      "12m 45s (- 0m 48s) (94000 94%) 0.3846\n",
      "12m 53s (- 0m 40s) (95000 95%) 0.4583\n",
      "13m 2s (- 0m 32s) (96000 96%) 0.3990\n",
      "13m 10s (- 0m 24s) (97000 97%) 0.4172\n",
      "13m 18s (- 0m 16s) (98000 98%) 0.3990\n",
      "13m 27s (- 0m 8s) (99000 99%) 0.4084\n",
      "13m 35s (- 0m 0s) (100000 100%) 0.3529\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256    #rnn's hidden_size\n",
    "in_embed_dim = 256   #input language word embedding dimension\n",
    "out_embed_dim = 256  #output language word embedding dimension\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, in_embed_dim).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, out_embed_dim, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 100000, print_every=1000, learning_rate=0.01)\n",
    "\n",
    "#因为每次迭代都是随机选一对句子，而且使用了teacher forcing，所以波动会较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 汤姆住在波士顿吗\n",
      "= 汤姆住在波士顿吗\n",
      "< 汤姆住在波士顿吗<EOS>\n",
      "\n",
      "> 汤姆认为我太年轻\n",
      "= 汤姆认为我太年轻\n",
      "< 汤姆认为我太太<EOS>\n",
      "\n",
      "> 这是你的书\n",
      "= 这是你的书\n",
      "< 这是你的书<EOS>\n",
      "\n",
      "> 你为甚么不锁门\n",
      "= 你为甚么不锁门\n",
      "< 你为甚么不锁门<EOS>\n",
      "\n",
      "> 他站在门后\n",
      "= 他站在门后\n",
      "< 他站在后后<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateInput(encoder, decoder):\n",
    "    input_sentence = ''\n",
    "    print('天不怕地不怕，就怕普通人讲四川话')\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('普通话> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, input_sentence)[0]\n",
    "            print('四川话:', ''.join(output_words[:-1]))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天不怕地不怕，就怕普通人讲四川话\n",
      "普通话> 你想做什么\n",
      "四川话: 你想做啥子\n",
      "普通话> 我要吃肉\n",
      "四川话: 我要吃肉\n",
      "普通话> 你怎么这么那个\n",
      "四川话: 你啷个这么那个\n",
      "普通话> 你动动我试试\n",
      "四川话: 你傍哈我告哈耶\n",
      "普通话> 我明天不上班\n",
      "四川话: 老子明天不上班\n",
      "普通话> q\n"
     ]
    }
   ],
   "source": [
    "evaluateInput(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求语料\n",
    "* 普通话-四川话平行语料，多多益善！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
